{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Problem statement\n",
    "\n",
    "100 candidates fill out a form of 10000 questions. Each candidate has a skill $s_i\\in(-3,3)$, and each question has a \n",
    "difficulty $d_j\\in(-3,3)$, both uniformly distributed. The probability for a student with skill $s_i$ to answer correctly \n",
    "a question of difficulty $d_j$ is:\n",
    "\n",
    "$$P(X_{ij}=1 | s_i, d_j) = \\sigma(s_i-d_j)$$\n",
    "\n",
    "where $\\sigma: x \\to \\frac{1}{1+e^{-x}}$ is the sigmoid function.\n",
    "\n",
    "One of the candidates is a **cheater**: with probability $\\frac{1}{2}$, they give the answer that they think is correct, \n",
    "but with probability $\\frac{1}{2}$ they *cheat* and look up the correct answer on the internet.\n",
    "\n",
    "By observing only the answers $X$ of all candidates to all questions, without knowing the skills of candidates nor the \n",
    "difficulties of questions, we must predict who the cheater is with the highest possible success rate. \n",
    "\n",
    "## Define the answering model\n",
    "\n",
    "If we want to estimate the skills of students and difficulties of questions from the answer data, we must first define\n",
    "a model describing how the three are related.\n",
    "\n",
    "Given two arrays of skills and difficulties, `ResponseModel` gives the probability $P(X_{ij}=1 | s_i, d_j)$ that each \n",
    "candidate $i$ answers correctly to question $j$:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Bernoulli\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ResponseModel(torch.nn.Module):\n",
    "    def __init__(self, n_students, n_questions):\n",
    "        super().__init__()\n",
    "        self.skill = torch.nn.Parameter(torch.zeros(n_students))\n",
    "        self.difficulty = torch.nn.Parameter(torch.zeros(n_questions))\n",
    "\n",
    "    def forward(self):\n",
    "        return Bernoulli(probs=torch.sigmoid(self.skill[:, None] - self.difficulty[None, :]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate some data\n",
    "\n",
    "We then generate some random skills, difficulties and answers according to this model, and then simulate the presence\n",
    "of a cheater among the students.  \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_data(n_students, n_questions):\n",
    "    # Sample students, questions, and answers\n",
    "    model = ResponseModel(n_students, n_students)\n",
    "    model.skill.data = -3 + 6*torch.rand(n_students)\n",
    "    model.difficulty.data = -3 + 6*torch.rand(n_questions)\n",
    "    answers = model().sample()\n",
    "    \n",
    "    # Pick a cheater, and make their answer correct with probability 1/2\n",
    "    cheater = torch.randint(high=n_students, size=(1,)).item()\n",
    "    is_cheating = Bernoulli(probs=0.5*torch.ones(n_questions)).sample().bool()\n",
    "    answers[cheater] = torch.where(is_cheating, torch.ones(n_questions), answers[cheater])\n",
    "    return model, answers, cheater\n",
    "\n",
    "true_model, answers, cheater = generate_data(n_students=100, n_questions=10000)\n",
    "print(\"Skill\", true_model.skill)\n",
    "print(\"Difficulty\", true_model.difficulty)\n",
    "print(\"Answers\", answers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fit the model by Maximum Likelihood Estimation\n",
    "\n",
    "### From MAP to MLE\n",
    "We need to estimate the skill $s$ of every student and difficulty $d$ of every question, given the observed answers $X$.\n",
    "The **Maximum A Posteriori (MAP)** estimate is then given by $\\text{arg}\\max_{s,d} P(s, d | X)$.\n",
    "\n",
    "By Bayes formula,\n",
    "\n",
    "$$P(s, d | X) = P(X|s, d)\\frac{P(s, d)}{P(X)}$$\n",
    "\n",
    "From the problem statement, we know that the prior $P(s, d)$ is uniform, so it does not depend on $s,d$, and neither \n",
    "does $P(X)$. Thus, the MAP estimate is equal to the **Maximum Likelihood Estimate (MLE)**:\n",
    "\n",
    "$$\\text{arg}\\max_{s,d} P(s, d | X) = \\text{arg}\\max_{s,d}P(X|s, d)$$\n",
    "\n",
    "And since answers are independent, we have\n",
    "$P(X|s, d) = \\prod_{i,j} P(X_{ij} | s_i, d_j)$\n",
    "\n",
    "And since $\\log$ is increasing, \n",
    "\n",
    "$$\\text{arg}\\max_{s,d}P(X|s, d) = \\text{arg}\\max_{s,d} \\log P(X|s, d) = \\text{arg}\\max_{s,d}\\sum_{i,j} \\log P(X_{ij} | s_i, d_j)$$\n",
    "\n",
    "Thus, the MAP estimate is given my minimising the **negative log-likelihood loss** $\\mathcal{L}(s,d) = - \\sum_{i,j} \\log P(X_{ij} | s_i, d_j)$ of the observed datapoints $X_{ij}$ according to our `ResponseModel`.\n",
    "\n",
    "### Centering the skills and difficulties\n",
    "\n",
    "The response model is invariant to translations of skills $s$ and difficulties $d$, and only sensitive to their differences $s-d$.\n",
    "Thus, we can freely offset them without affecting the induced answers probabilities.\n",
    "Since the true difficulties and skills are centered around $0$, we can enforce this for our estimates by adding a\n",
    "term to the loss, penalizing the squared mean difficulty:\n",
    "\n",
    "$$\\mathcal{L}(s,d) = - \\sum_{i,j} \\log P(X_{ij} | s_i, d_j) + \\alpha \\left(\\sum_j d_j\\right)^2$$\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def fit(model, answers, epochs, alpha=1.0, see_progress=True):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    if torch.cuda.is_available():\n",
    "        model, answers = model.cuda(), answers.cuda()\n",
    "    losses = []\n",
    "    for _ in trange(epochs) if see_progress else range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        distribution = model()\n",
    "        loss = -distribution.log_prob(answers).mean() + alpha*torch.pow(model.difficulty.mean(), 2)\n",
    "        losses.append(loss.detach())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    model, answers = model.cpu(), answers.cpu()\n",
    "    return model, losses\n",
    "\n",
    "model = ResponseModel(n_students=answers.shape[0], n_questions=answers.shape[1])\n",
    "model, losses = fit(model, answers, epochs=1000, alpha=1e-2)\n",
    "plt.plot(losses, label=\"loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualize estimation errors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.hist((model.difficulty - true_model.difficulty).detach().numpy(), density=True, label=\"difficulty error\")\n",
    "plt.hist((model.skill - true_model.skill).detach().numpy(), density=True, label=\"skill error\")\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You may notice that only one single student has a skill largely overestimated... Who could that be?\n",
    "\n",
    "## Predict the cheater, with lowest answers likelihood"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "log_probs = model().log_prob(answers).sum(axis=1).detach().numpy()\n",
    "predicted_cheater = log_probs.argmin()\n",
    "print(f\"Predicted cheater: {predicted_cheater}. True cheater: {cheater}\")\n",
    "print(f\"Predicted skill: {model.skill[predicted_cheater]}, true skill: {true_model.skill[cheater]}\")\n",
    "\n",
    "plt.scatter(range(log_probs.size), log_probs, marker='.')\n",
    "plt.scatter(x=cheater, y=log_probs[cheater], c='r', marker='o', label=\"true cheater\")\n",
    "plt.scatter(x=predicted_cheater, y=log_probs[predicted_cheater], marker='o', s=80,\n",
    "            facecolor='none', edgecolor='b', label=\"predicted cheater\")\n",
    "plt.ylabel(\"log prob of answers\")\n",
    "plt.xlabel(\"student\")\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Repeat and measure success rate\n",
    "*It should be enabled by default, but be sure to select GPU in* `Execution / Execution Type` *to get a faster training time (~4mn).* "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "samples = 100\n",
    "successes = 0\n",
    "for _ in trange(samples):\n",
    "    true_model, answers, cheater = generate_data(n_students=100, n_questions=10000)\n",
    "    model = ResponseModel(n_students=answers.shape[0], n_questions=answers.shape[1])\n",
    "    model, _ = fit(model, answers, epochs=1000, alpha=1e-2, see_progress=False)\n",
    "    predicted_cheater = model().log_prob(answers).sum(axis=1).detach().numpy().argmin()\n",
    "    successes += predicted_cheater == cheater\n",
    "print()\n",
    "print(f\"Success rate: {successes/samples*100:.1f}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Unfortunately, the success rate is too low, and does not reach the desired target of 84%.\n",
    "This is because our response model does not account for the presence of the cheater, and its effects on the distribution of answers.\n",
    "\n",
    "Let us try to include this information in our model.\n",
    "\n",
    "## Include the cheater in the item response model\n",
    "\n",
    "We add an additional parameter to the model: the (log-)probability that any student is the cheater.  "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ResponseModelWithCheater(ResponseModel):\n",
    "    def __init__(self, n_students, n_questions):\n",
    "        super().__init__(n_students, n_questions)\n",
    "        self.cheater_logits = torch.nn.Parameter(torch.zeros(n_students))\n",
    "\n",
    "    def forward(self):\n",
    "        cheater_prob = F.softmax(self.cheater_logits)[:, None]\n",
    "        answer_no_cheat = torch.sigmoid(self.skill[:, None] - self.difficulty[None, :])\n",
    "        answer_cheat = 0.5 * torch.sigmoid(self.skill[:, None] - self.difficulty[None, :]) + 0.5 * 1\n",
    "        return Bernoulli(probs=cheater_prob*answer_cheat + (1-cheater_prob)*answer_no_cheat)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fit the updated model with cheater"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = ResponseModelWithCheater(n_students=answers.shape[0], n_questions=answers.shape[1])\n",
    "model, losses = fit(model, answers, epochs=1000, alpha=1e-2)\n",
    "plt.plot(losses, label=\"loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot the cheater probabilities, and predicted cheater"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cheater_probs = F.softmax(model.cheater_logits).detach().numpy()\n",
    "plt.axvline(x=cheater, c='r')\n",
    "plt.plot(cheater_probs)\n",
    "plt.xlabel(\"student\")\n",
    "plt.ylabel(\"cheater probability\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Repeat and measure success rate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "samples = 100\n",
    "successes = 0\n",
    "for _ in trange(samples):\n",
    "    true_model, answers, cheater = generate_data(n_students=100, n_questions=10000)\n",
    "    model = ResponseModelWithCheater(n_students=answers.shape[0], n_questions=answers.shape[1])\n",
    "    model, _ = fit(model, answers, epochs=1000, alpha=1e-2, see_progress=False)\n",
    "    predicted_cheater = F.softmax(model.cheater_logits).detach().numpy().argmax()\n",
    "    successes += predicted_cheater == cheater\n",
    "print()\n",
    "print(f\"Success rate: {successes/samples*100:.1f}%\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  },
  "colab": {
   "name": "Cheater",
   "provenance": []
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}