{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Define the item response model\n",
    "\n",
    "The probability for a student with skill $s$ to answer correctly a question of difficulty $d$ is:\n",
    "\n",
    "$$P(X=1 | s, d) = \\sigma(s-d)$$\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Bernoulli\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ResponseModel(torch.nn.Module):\n",
    "    def __init__(self, n_students, n_questions):\n",
    "        super().__init__()\n",
    "        self.skill = torch.nn.Parameter(torch.zeros(n_students))\n",
    "        self.difficulty = torch.nn.Parameter(torch.zeros(n_questions))\n",
    "\n",
    "    def forward(self):\n",
    "        return Bernoulli(probs=torch.sigmoid(self.skill[:, None] - self.difficulty[None, :]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate some data\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_data(n_students, n_questions):\n",
    "    # Sample students, questions, and answers\n",
    "    model = ResponseModel(n_students, n_students)\n",
    "    model.skill.data = -3 + 6*torch.rand(n_students)\n",
    "    model.difficulty.data = -3 + 6*torch.rand(n_questions)\n",
    "    answers = model().sample()\n",
    "    \n",
    "    # Pick a cheater, and make their answer correct with probability 1/2\n",
    "    cheater = torch.randint(high=n_students, size=(1,)).item()\n",
    "    is_cheating = Bernoulli(probs=0.5*torch.ones(n_questions)).sample().bool()\n",
    "    answers[cheater] = torch.where(is_cheating, torch.ones(n_questions), answers[cheater])\n",
    "    return model, answers, cheater\n",
    "\n",
    "true_model, answers, cheater = generate_data(n_students=100, n_questions=10000)\n",
    "print(\"Skill\", true_model.skill)\n",
    "print(\"Difficulty\", true_model.difficulty)\n",
    "print(\"Answers\", answers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fit the model by Maximum Likelihood Estimation\n",
    "Optimized by Stochastic Gradient Descent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def fit(model, answers, epochs, alpha=1.0, see_progress=True):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    if torch.cuda.is_available():\n",
    "        model, answers = model.cuda(), answers.cuda()\n",
    "    losses = []\n",
    "    for _ in trange(epochs) if see_progress else range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        distribution = model()\n",
    "        loss = -distribution.log_prob(answers).mean() + alpha*torch.pow(model.difficulty.mean(), 2)\n",
    "        losses.append(loss.detach())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    model, answers = model.cpu(), answers.cpu()\n",
    "    return model, losses\n",
    "\n",
    "model = ResponseModel(n_students=answers.shape[0], n_questions=answers.shape[1])\n",
    "model, losses = fit(model, answers, epochs=1000, alpha=1e-2)\n",
    "plt.plot(losses, label=\"loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualize estimation errors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.hist((true_model.difficulty - model.difficulty).detach().numpy(), density=True, label=\"difficulty error\")\n",
    "plt.hist((true_model.skill - model.skill).detach().numpy(), density=True, label=\"skill error\")\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You may notice that one single student has a high skill error... Who could that be?\n",
    "\n",
    "## Predict the cheater, with lowest answers likelihood"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "log_probs = model().log_prob(answers).sum(axis=1).detach().numpy()\n",
    "predicted_cheater = log_probs.argmin()\n",
    "print(f\"Predicted cheater: {predicted_cheater}. True cheater: {cheater}\")\n",
    "print(f\"Predicted skill: {model.skill[predicted_cheater]}, true skill: {true_model.skill[cheater]}\")\n",
    "\n",
    "plt.scatter(range(log_probs.size), log_probs, marker='.')\n",
    "plt.scatter(x=cheater, y=log_probs[cheater], c='r', marker='o', label=\"true cheater\")\n",
    "plt.scatter(x=predicted_cheater, y=log_probs[predicted_cheater], marker='o', s=80,\n",
    "            facecolor='none', edgecolor='b', label=\"predicted cheater\")\n",
    "plt.ylabel(\"log prob of answers\")\n",
    "plt.xlabel(\"student\")\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Repeat and measure success rate\n",
    "*It should be enabled by default, but be sure to select GPU in* `Execution / Execution Type` *to get a faster training time (~4mn).* "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "samples = 100\n",
    "successes = 0\n",
    "for _ in trange(samples):\n",
    "    true_model, answers, cheater = generate_data(n_students=100, n_questions=10000)\n",
    "    model = ResponseModel(n_students=answers.shape[0], n_questions=answers.shape[1])\n",
    "    model, _ = fit(model, answers, epochs=1000, alpha=1e-2, see_progress=False)\n",
    "    predicted_cheater = model().log_prob(answers).sum(axis=1).detach().numpy().argmin()\n",
    "    successes += predicted_cheater == cheater\n",
    "print()\n",
    "print(f\"Success rate: {successes/samples*100:.1f}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Unfortunately, the success rate is too low, and does not reach the desired target of 84%.\n",
    "This is because our response model does not account for the presence of the cheater, and its effects on the distribution of answers.\n",
    "\n",
    "Let us try to include this information in our model.\n",
    "\n",
    "## Include the cheater in the item response model\n",
    "\n",
    "We add an additional parameter to the model: the (log-)probability that any student is the cheater.  "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ResponseModelWithCheater(ResponseModel):\n",
    "    def __init__(self, n_students, n_questions):\n",
    "        super().__init__(n_students, n_questions)\n",
    "        self.cheater_logits = torch.nn.Parameter(torch.zeros(n_students))\n",
    "\n",
    "    def forward(self):\n",
    "        cheater_prob = F.softmax(self.cheater_logits)[:, None]\n",
    "        answer_no_cheat = torch.sigmoid(self.skill[:, None] - self.difficulty[None, :])\n",
    "        answer_cheat = 0.5 * torch.sigmoid(self.skill[:, None] - self.difficulty[None, :]) + 0.5 * 1\n",
    "        return Bernoulli(probs=cheater_prob*answer_cheat + (1-cheater_prob)*answer_no_cheat)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fit the updated model with cheater"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = ResponseModelWithCheater(n_students=answers.shape[0], n_questions=answers.shape[1])\n",
    "model, losses = fit(model, answers, epochs=1000, alpha=1e-2)\n",
    "plt.plot(losses, label=\"loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot the cheater probability"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cheater_probs = F.softmax(model.cheater_logits).detach().numpy()\n",
    "plt.axvline(x=cheater, c='r')\n",
    "plt.plot(cheater_probs)\n",
    "plt.xlabel(\"student\")\n",
    "plt.ylabel(\"cheater probability\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Repeat and measure success rate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "samples = 100\n",
    "successes = 0\n",
    "for _ in trange(samples):\n",
    "    true_model, answers, cheater = generate_data(n_students=100, n_questions=10000)\n",
    "    model = ResponseModelWithCheater(n_students=answers.shape[0], n_questions=answers.shape[1])\n",
    "    model, _ = fit(model, answers, epochs=1000, alpha=1e-2, see_progress=False)\n",
    "    predicted_cheater = F.softmax(model.cheater_logits).detach().numpy().argmax()\n",
    "    successes += predicted_cheater == cheater\n",
    "print()\n",
    "print(f\"Success rate: {successes/samples*100:.1f}%\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  },
  "colab": {
    "name": "Cheater",
    "provenance": []
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}